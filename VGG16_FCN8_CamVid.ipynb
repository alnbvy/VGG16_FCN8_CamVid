{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ3g9dJxSxmN"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aifz2907kxYN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "#import seaborn as sns\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sLz8mI2S62W"
   },
   "source": [
    "## Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1Z5V1XMBNJso"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: gdown: command not found\n"
     ]
    }
   ],
   "source": [
    "# # download the dataset (zipped file)\n",
    "# !gdown --id 0B0d9ZiqAgFkiOHR1NTJhWVJMNEU -O /tmp/fcnn-dataset.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fjQKhE_3WFLF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing download.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile download.sh\n",
    "\n",
    "#!/bin/bash\n",
    "fileid=\"0B0d9ZiqAgFkiOHR1NTJhWVJMNEU\"\n",
    "filename=\"/tmp/fcnn-dataset.zip\"\n",
    "html=`curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\"`\n",
    "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&`echo ${html}|grep -Po '(confirm=[a-zA-Z0-9\\-_]+)'`&id=${fileid}\" -o ${filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Nvl1E05nWMoc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  119M  100  119M    0     0  5512k      0  0:00:22  0:00:22 --:--:-- 5195k\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Please only run this if downloading with gdown did not work.\n",
    "# This will run the script created above.\n",
    "!bash download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-vPfaklRWC_L"
   },
   "outputs": [],
   "source": [
    "# extract the downloaded dataset to a local directory: /tmp/fcnn\n",
    "local_zip = '/tmp/fcnn-dataset.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp/fcnn')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OlK0lVK_-Z46"
   },
   "outputs": [],
   "source": [
    "# pixel labels in the video frames\n",
    "class_names = ['sky', 'building','column/pole', 'road', 'side walk', 'vegetation', 'traffic light', 'fence', 'vehicle', 'pedestrian', 'byciclist', 'void']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mi0NcGqESgj2"
   },
   "source": [
    "## Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Lsc-_7Xu_twj"
   },
   "outputs": [],
   "source": [
    "def map_filename_to_image_and_mask(t_filename, a_filename, height=224, width=224):\n",
    "  '''\n",
    "  Preprocesses the dataset by:\n",
    "    * resizing the input image and label maps\n",
    "    * normalizing the input image pixels\n",
    "    * reshaping the label maps from (height, width, 1) to (height, width, 12)\n",
    "\n",
    "  Args:\n",
    "    t_filename (string) -- path to the raw input image\n",
    "    a_filename (string) -- path to the raw annotation (label map) file\n",
    "    height (int) -- height in pixels to resize to\n",
    "    width (int) -- width in pixels to resize to\n",
    "\n",
    "  Returns:\n",
    "    image (tensor) -- preprocessed image\n",
    "    annotation (tensor) -- preprocessed annotation\n",
    "  '''\n",
    "\n",
    "  # Convert image and mask files to tensors \n",
    "  img_raw = tf.io.read_file(t_filename)\n",
    "  anno_raw = tf.io.read_file(a_filename)\n",
    "  image = tf.image.decode_jpeg(img_raw)\n",
    "  annotation = tf.image.decode_jpeg(anno_raw)\n",
    " \n",
    "  # Resize image and segmentation mask\n",
    "  image = tf.image.resize(image, (height, width,))\n",
    "  annotation = tf.image.resize(annotation, (height, width,))\n",
    "  image = tf.reshape(image, (height, width, 3,))\n",
    "  annotation = tf.cast(annotation, dtype=tf.int32)\n",
    "  annotation = tf.reshape(annotation, (height, width, 1,))\n",
    "  stack_list = []\n",
    "\n",
    "  # Reshape segmentation masks\n",
    "  for c in range(len(class_names)):\n",
    "    mask = tf.equal(annotation[:,:,0], tf.constant(c))\n",
    "    stack_list.append(tf.cast(mask, dtype=tf.int32))\n",
    "  \n",
    "  annotation = tf.stack(stack_list, axis=2)\n",
    "\n",
    "  # Normalize pixels in the input image\n",
    "  image = image/127.5\n",
    "  image -= 1\n",
    "\n",
    "  return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k5Z8UbTQGRcC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations_prepped_test   images_prepped_test\n",
      "annotations_prepped_train  images_prepped_train\n"
     ]
    }
   ],
   "source": [
    "# show folders inside the dataset you downloaded\n",
    "!ls /tmp/fcnn/dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S8YE6w9g-ZEF"
   },
   "outputs": [],
   "source": [
    "# Utilities for preparing the datasets\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def get_dataset_slice_paths(image_dir, label_map_dir):\n",
    "  '''\n",
    "  generates the lists of image and label map paths\n",
    "  \n",
    "  Args:\n",
    "    image_dir (string) -- path to the input images directory\n",
    "    label_map_dir (string) -- path to the label map directory\n",
    "\n",
    "  Returns:\n",
    "    image_paths (list of strings) -- paths to each image file\n",
    "    label_map_paths (list of strings) -- paths to each label map\n",
    "  '''\n",
    "  image_file_list = os.listdir(image_dir)\n",
    "  label_map_file_list = os.listdir(label_map_dir)\n",
    "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "  label_map_paths = [os.path.join(label_map_dir, fname) for fname in label_map_file_list]\n",
    "\n",
    "  return image_paths, label_map_paths\n",
    "\n",
    "\n",
    "def get_training_dataset(image_paths, label_map_paths):\n",
    "  '''\n",
    "  Prepares shuffled batches of the training set.\n",
    "  \n",
    "  Args:\n",
    "    image_paths (list of strings) -- paths to each image file in the train set\n",
    "    label_map_paths (list of strings) -- paths to each label map in the train set\n",
    "\n",
    "  Returns:\n",
    "    tf Dataset containing the preprocessed train set\n",
    "  '''\n",
    "  training_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "  training_dataset = training_dataset.map(map_filename_to_image_and_mask)\n",
    "  training_dataset = training_dataset.shuffle(100, reshuffle_each_iteration=True)\n",
    "  training_dataset = training_dataset.batch(BATCH_SIZE)\n",
    "  training_dataset = training_dataset.repeat()\n",
    "  training_dataset = training_dataset.prefetch(-1)\n",
    "\n",
    "  return training_dataset\n",
    "\n",
    "\n",
    "def get_validation_dataset(image_paths, label_map_paths):\n",
    "  '''\n",
    "  Prepares batches of the validation set.\n",
    "  \n",
    "  Args:\n",
    "    image_paths (list of strings) -- paths to each image file in the val set\n",
    "    label_map_paths (list of strings) -- paths to each label map in the val set\n",
    "\n",
    "  Returns:\n",
    "    tf Dataset containing the preprocessed validation set\n",
    "  '''\n",
    "  validation_dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_map_paths))\n",
    "  validation_dataset = validation_dataset.map(map_filename_to_image_and_mask)\n",
    "  validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "  validation_dataset = validation_dataset.repeat()  \n",
    "\n",
    "  return validation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "skVGwEPmeiwz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function map_filename_to_image_and_mask at 0x2ab8bf8c6b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function map_filename_to_image_and_mask at 0x2ab8bf8c6b80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 16:08:54.896623: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-03 16:08:54.903945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-03 16:08:54.927104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-07-03 16:08:54.927164: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-03 16:08:55.031932: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-03 16:08:55.032077: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-03 16:08:55.150993: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-03 16:08:55.273204: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-03 16:08:55.362294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-03 16:08:55.406747: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-03 16:08:55.524063: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-03 16:08:55.525151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-03 16:08:55.526124: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-03 16:08:55.526644: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-03 16:08:55.527369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:18:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-07-03 16:08:55.527437: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-03 16:08:55.527491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-03 16:08:55.527524: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-03 16:08:55.527556: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-03 16:08:55.527587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-03 16:08:55.527618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-03 16:08:55.527649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-03 16:08:55.527681: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-03 16:08:55.528507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-07-03 16:08:55.528572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-07-03 16:08:56.726818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-03 16:08:56.726862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-07-03 16:08:56.726873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-07-03 16:08:56.727902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:18:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "# get the paths to the images\n",
    "training_image_paths, training_label_map_paths = get_dataset_slice_paths('/tmp/fcnn/dataset1/images_prepped_train/','/tmp/fcnn/dataset1/annotations_prepped_train/')\n",
    "validation_image_paths, validation_label_map_paths = get_dataset_slice_paths('/tmp/fcnn/dataset1/images_prepped_test/','/tmp/fcnn/dataset1/annotations_prepped_test/')\n",
    "\n",
    "# generate the train and val sets\n",
    "training_dataset = get_training_dataset(training_image_paths, training_label_map_paths)\n",
    "validation_dataset = get_validation_dataset(validation_image_paths, validation_label_map_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 16:08:56.968542: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-03 16:08:56.971703: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 224, 224, 3)\n",
      "(64, 224, 224, 12)\n"
     ]
    }
   ],
   "source": [
    "for image, label in training_dataset.take(1):\n",
    "    pass\n",
    "print(image.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuciRFRpajTe"
   },
   "outputs": [],
   "source": [
    "# # generate a list that contains one color for each class\n",
    "# colors = sns.color_palette(None, len(class_names))\n",
    "\n",
    "# # print class name - normalized RGB tuple pairs\n",
    "# # the tuple values will be multiplied by 255 in the helper functions later\n",
    "# # to convert to the (0,0,0) to (255,255,255) RGB values you might be familiar with\n",
    "# for class_name, color in zip(class_names, colors):\n",
    "#   print(f'{class_name} -- {color}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d46YCbvPafbp"
   },
   "outputs": [],
   "source": [
    "# # Visualization Utilities\n",
    "\n",
    "# def fuse_with_pil(images):\n",
    "#   '''\n",
    "#   Creates a blank image and pastes input images\n",
    "\n",
    "#   Args:\n",
    "#     images (list of numpy arrays) - numpy array representations of the images to paste\n",
    "  \n",
    "#   Returns:\n",
    "#     PIL Image object containing the images\n",
    "#   '''\n",
    "\n",
    "#   widths = (image.shape[1] for image in images)\n",
    "#   heights = (image.shape[0] for image in images)\n",
    "#   total_width = sum(widths)\n",
    "#   max_height = max(heights)\n",
    "\n",
    "#   new_im = PIL.Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "#   x_offset = 0\n",
    "#   for im in images:\n",
    "#     pil_image = PIL.Image.fromarray(np.uint8(im))\n",
    "#     new_im.paste(pil_image, (x_offset,0))\n",
    "#     x_offset += im.shape[1]\n",
    "  \n",
    "#   return new_im\n",
    "\n",
    "\n",
    "# def give_color_to_annotation(annotation):\n",
    "#   '''\n",
    "#   Converts a 2-D annotation to a numpy array with shape (height, width, 3) where\n",
    "#   the third axis represents the color channel. The label values are multiplied by\n",
    "#   255 and placed in this axis to give color to the annotation\n",
    "\n",
    "#   Args:\n",
    "#     annotation (numpy array) - label map array\n",
    "  \n",
    "#   Returns:\n",
    "#     the annotation array with an additional color channel/axis\n",
    "#   '''\n",
    "#   seg_img = np.zeros( (annotation.shape[0],annotation.shape[1], 3) ).astype('float')\n",
    "  \n",
    "#   for c in range(12):\n",
    "#     segc = (annotation == c)\n",
    "#     seg_img[:,:,0] += segc*( colors[c][0] * 255.0)\n",
    "#     seg_img[:,:,1] += segc*( colors[c][1] * 255.0)\n",
    "#     seg_img[:,:,2] += segc*( colors[c][2] * 255.0)\n",
    "  \n",
    "#   return seg_img\n",
    "\n",
    "\n",
    "# def show_predictions(image, labelmaps, titles, iou_list, dice_score_list):\n",
    "#   '''\n",
    "#   Displays the images with the ground truth and predicted label maps\n",
    "\n",
    "#   Args:\n",
    "#     image (numpy array) -- the input image\n",
    "#     labelmaps (list of arrays) -- contains the predicted and ground truth label maps\n",
    "#     titles (list of strings) -- display headings for the images to be displayed\n",
    "#     iou_list (list of floats) -- the IOU values for each class\n",
    "#     dice_score_list (list of floats) -- the Dice Score for each vlass\n",
    "#   '''\n",
    "\n",
    "#   true_img = give_color_to_annotation(labelmaps[1])\n",
    "#   pred_img = give_color_to_annotation(labelmaps[0])\n",
    "\n",
    "#   image = image + 1\n",
    "#   image = image * 127.5\n",
    "#   images = np.uint8([image, pred_img, true_img])\n",
    "\n",
    "#   metrics_by_id = [(idx, iou, dice_score) for idx, (iou, dice_score) in enumerate(zip(iou_list, dice_score_list)) if iou > 0.0]\n",
    "#   metrics_by_id.sort(key=lambda tup: tup[1], reverse=True)  # sorts in place\n",
    "  \n",
    "#   display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx], iou, dice_score) for idx, iou, dice_score in metrics_by_id]\n",
    "#   display_string = \"\\n\\n\".join(display_string_list) \n",
    "\n",
    "#   plt.figure(figsize=(15, 4))\n",
    "\n",
    "#   for idx, im in enumerate(images):\n",
    "#     plt.subplot(1, 3, idx+1)\n",
    "#     if idx == 1:\n",
    "#       plt.xlabel(display_string)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.title(titles[idx], fontsize=12)\n",
    "#     plt.imshow(im)\n",
    "\n",
    "\n",
    "# def show_annotation_and_image(image, annotation):\n",
    "#   '''\n",
    "#   Displays the image and its annotation side by side\n",
    "\n",
    "#   Args:\n",
    "#     image (numpy array) -- the input image\n",
    "#     annotation (numpy array) -- the label map\n",
    "#   '''\n",
    "#   new_ann = np.argmax(annotation, axis=2)\n",
    "#   seg_img = give_color_to_annotation(new_ann)\n",
    "  \n",
    "#   image = image + 1\n",
    "#   image = image * 127.5\n",
    "#   image = np.uint8(image)\n",
    "#   images = [image, seg_img]\n",
    "  \n",
    "#   images = [image, seg_img]\n",
    "#   fused_img = fuse_with_pil(images)\n",
    "#   plt.imshow(fused_img)\n",
    "\n",
    "\n",
    "# def list_show_annotation(dataset):\n",
    "#   '''\n",
    "#   Displays images and its annotations side by side\n",
    "\n",
    "#   Args:\n",
    "#     dataset (tf Dataset) - batch of images and annotations\n",
    "#   '''\n",
    "\n",
    "#   ds = dataset.unbatch()\n",
    "#   ds = ds.shuffle(buffer_size=100)\n",
    "\n",
    "#   plt.figure(figsize=(25, 15))\n",
    "#   plt.title(\"Images And Annotations\")\n",
    "#   plt.subplots_adjust(bottom=0.1, top=0.9, hspace=0.05)\n",
    "\n",
    "#   # we set the number of image-annotation pairs to 9\n",
    "#   # feel free to make this a function parameter if you want\n",
    "#   for idx, (image, annotation) in enumerate(ds.take(9)):\n",
    "#     plt.subplot(3, 3, idx + 1)\n",
    "#     plt.yticks([])\n",
    "#     plt.xticks([])\n",
    "#     show_annotation_and_image(image.numpy(), annotation.numpy())\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK621Jm8bJyj"
   },
   "source": [
    "\n",
    "Please run the cells below to see sample images from the train and validation sets. You will see the image and the label maps side side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFO_hIhLWYT4"
   },
   "outputs": [],
   "source": [
    "# list_show_annotation(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdgVkp8wZua0"
   },
   "outputs": [],
   "source": [
    "# list_show_annotation(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHB1BGmF965d"
   },
   "source": [
    "### Define Pooling Block of VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pL578pjdmXXf"
   },
   "outputs": [],
   "source": [
    "def block(x, n_convs, filters, kernel_size, activation, pool_size, pool_stride, block_name):\n",
    "  '''\n",
    "  Defines a block in the VGG network.\n",
    "\n",
    "  Args:\n",
    "    x (tensor) -- input image\n",
    "    n_convs (int) -- number of convolution layers to append\n",
    "    filters (int) -- number of filters for the convolution layers\n",
    "    activation (string or object) -- activation to use in the convolution\n",
    "    pool_size (int) -- size of the pooling layer\n",
    "    pool_stride (int) -- stride of the pooling layer\n",
    "    block_name (string) -- name of the block\n",
    "\n",
    "  Returns:\n",
    "    tensor containing the max-pooled output of the convolutions\n",
    "  '''\n",
    "\n",
    "  for i in range(n_convs):\n",
    "      x = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding='same', name=\"{}_conv{}\".format(block_name, i + 1))(x)\n",
    "    \n",
    "  x = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_stride, name=\"{}_pool{}\".format(block_name, i+1 ))(x)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lm_8Jp4PbVV5"
   },
   "source": [
    "### Download VGG weights\n",
    "\n",
    "First, please run the cell below to get pre-trained weights for VGG-16. You will load this in the next section when you build the encoder network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out https://github.com/fchollet/deep-learning-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vKPpXapoYxAc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /packages/7x/anaconda3/5.3.0/lib/libuuid.so.1: no version information available (required by wget)\n",
      "--2022-07-03 16:09:37--  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/64878964/b09fedd4-5983-11e6-8f9f-904ea400969a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220703%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220703T230938Z&X-Amz-Expires=300&X-Amz-Signature=516d7976dfa6efb5a502adc7c9e3a08efa5e607cc2e474a6dba8bf30f705f68b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream [following]\n",
      "--2022-07-03 16:09:38--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/64878964/b09fedd4-5983-11e6-8f9f-904ea400969a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220703%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220703T230938Z&X-Amz-Expires=300&X-Amz-Signature=516d7976dfa6efb5a502adc7c9e3a08efa5e607cc2e474a6dba8bf30f705f68b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58889256 (56M) [application/octet-stream]\n",
      "Saving to: ‘vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
      "\n",
      "100%[======================================>] 58,889,256  29.0MB/s   in 1.9s   \n",
      "\n",
      "2022-07-03 16:09:40 (29.0 MB/s) - ‘vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [58889256/58889256]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the weights\n",
    "!wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "\n",
    "# assign to a variable\n",
    "vgg_weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLeQCxf99_tn"
   },
   "source": [
    "### Define VGG-16\n",
    "\n",
    "You can build the encoder as shown below. \n",
    "\n",
    "* You will create 5 blocks with increasing number of filters at each stage. \n",
    "* The number of convolutions, filters, kernel size, activation, pool size and pool stride will remain constant.\n",
    "* You will load the pretrained weights after creating the VGG 16 network.\n",
    "* Additional convolution layers will be appended to extract more features.\n",
    "* The output will contain the output of the last layer and the previous four convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Z4_WZnAmoOnZ"
   },
   "outputs": [],
   "source": [
    "def VGG_16(image_input):\n",
    "  '''\n",
    "  This function defines the VGG encoder.\n",
    "\n",
    "  Args:\n",
    "    image_input (tensor) - batch of images\n",
    "\n",
    "  Returns:\n",
    "    tuple of tensors - output of all encoder blocks plus the final convolution layer\n",
    "  '''\n",
    "\n",
    "  # create 5 blocks with increasing filters at each stage. \n",
    "  # you will save the output of each block (i.e. p1, p2, p3, p4, p5). \"p\" stands for the pooling layer.\n",
    "  x = block(image_input,n_convs=2, filters=64, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block1')\n",
    "  p1= x\n",
    "\n",
    "  x = block(x,n_convs=2, filters=128, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block2')\n",
    "  p2 = x\n",
    "\n",
    "  x = block(x,n_convs=3, filters=256, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block3')\n",
    "  p3 = x\n",
    "\n",
    "  x = block(x,n_convs=3, filters=512, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block4')\n",
    "  p4 = x\n",
    "\n",
    "  x = block(x,n_convs=3, filters=512, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block5')\n",
    "  p5 = x\n",
    "\n",
    "  # create the vgg model\n",
    "  vgg  = tf.keras.Model(image_input , p5)\n",
    "\n",
    "  # load the pretrained weights you downloaded earlier\n",
    "  vgg.load_weights(vgg_weights_path) \n",
    "\n",
    "  # number of filters for the output convolutional layers\n",
    "  n = 4096\n",
    "\n",
    "  # our input images are 224x224 pixels so they will be downsampled to 7x7 after the pooling layers above.\n",
    "  # we can extract more features by chaining two more convolution layers.\n",
    "  c6 = tf.keras.layers.Conv2D( n , ( 7 , 7 ) , activation='relu' , padding='same', name=\"conv6\")(p5)\n",
    "  c7 = tf.keras.layers.Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"conv7\")(c6)\n",
    "\n",
    "  # return the outputs at each stage. you will only need two of these in this particular exercise \n",
    "  # but we included it all in case you want to experiment with other types of decoders.\n",
    "  return (p1, p2, p3, p4, c7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(224,224,3,))\n",
    "convs = VGG_16(image_input=inputs)\n",
    "model_vgg_only = tf.keras.Model(inputs=inputs, outputs=convs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool2 (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool2 (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool3 (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool3 (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool3 (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 7, 7, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 7, 7, 4096)        16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_vgg_only.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hX2V0E2gs-ZQ"
   },
   "outputs": [],
   "source": [
    "def fcn8_decoder(convs, n_classes):\n",
    "  '''\n",
    "  Defines the FCN 8 decoder.\n",
    "\n",
    "  Args:\n",
    "    convs (tuple of tensors) - output of the encoder network\n",
    "    n_classes (int) - number of classes\n",
    "\n",
    "  Returns:\n",
    "    tensor with shape (height, width, n_classes) containing class probabilities\n",
    "  '''\n",
    "\n",
    "  # unpack the output of the encoder\n",
    "  f1, f2, f3, f4, f5 = convs\n",
    "  \n",
    "  # H = (H1 - 1)*stride + HF - 2*padding  (for Conv2DTranspose)\n",
    "\n",
    "  # upsample the output of the encoder then crop extra pixels that were introduced\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5) # Output is 16*16\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f4 # size was 14*14\n",
    "  o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same'))(o2) # 14*14\n",
    "\n",
    "  # add the results of the upsampling and pool 4 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2]) # 14*14\n",
    "\n",
    "  # upsample the resulting tensor of the operation you just did\n",
    "  o = (tf.keras.layers.Conv2DTranspose( n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False ))(o) # 30 * 30\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o) # 28*28\n",
    "\n",
    "  # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f3\n",
    "  o2 = ( tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same'))(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 3 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2]) # 28*28\n",
    "  \n",
    "  # upsample up to the size of the original image\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o) # 224*224\n",
    "\n",
    "  # append a softmax to get the class probabilities\n",
    "  o = (tf.keras.layers.Activation('softmax'))(o)\n",
    "\n",
    "  return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyn3xXSf_Ogl"
   },
   "source": [
    "### Define Final Model\n",
    "\n",
    "You can now build the final model by connecting the encoder and decoder blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "T29n8_dbuZNm"
   },
   "outputs": [],
   "source": [
    "def segmentation_model():\n",
    "  '''\n",
    "  Defines the final segmentation model by chaining together the encoder and decoder.\n",
    "\n",
    "  Returns:\n",
    "    keras Model that connects the encoder and decoder networks of the segmentation model\n",
    "  '''\n",
    "  \n",
    "  inputs = tf.keras.layers.Input(shape=(224,224,3,))\n",
    "  convs = VGG_16(image_input=inputs)\n",
    "  outputs = fcn8_decoder(convs, 12)\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "  \n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "_w8qNGG1vQHZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 224, 224, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 224, 224, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool2 (MaxPooling2D)     (None, 112, 112, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 112, 112, 128 73856       block1_pool2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 112, 112, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool2 (MaxPooling2D)     (None, 56, 56, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 56, 56, 256)  295168      block2_pool2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 56, 56, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool3 (MaxPooling2D)     (None, 28, 28, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 28, 28, 512)  1180160     block3_pool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool3 (MaxPooling2D)     (None, 14, 14, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 14, 14, 512)  2359808     block4_pool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 14, 14, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool3 (MaxPooling2D)     (None, 7, 7, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 7, 7, 4096)   102764544   block5_pool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv7 (Conv2D)                  (None, 7, 7, 4096)   16781312    conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 16, 16, 12)   786432      conv7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d (Cropping2D)         (None, 14, 14, 12)   0           conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 14, 12)   6156        block4_pool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 14, 14, 12)   0           cropping2d[0][0]                 \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 30, 30, 12)   2304        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)       (None, 28, 28, 12)   0           conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 12)   3084        block3_pool3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 28, 28, 12)   0           cropping2d_1[0][0]               \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 224, 224, 12) 9216        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 224, 224, 12) 0           conv2d_transpose_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 135,067,736\n",
      "Trainable params: 135,067,736\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate the model and see how it looks\n",
    "model = segmentation_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dekOKLw0_Rgg"
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ZpWpp8h4g_rE"
   },
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=1E-2, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8HoZwpGWhMB-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-03 16:10:20.687186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-03 16:10:22.466999: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 13s 2s/step - loss: 2.9521 - accuracy: 0.0854 - val_loss: 2.4835 - val_accuracy: 0.0958\n",
      "Epoch 2/170\n",
      "5/5 [==============================] - 6s 832ms/step - loss: 2.4829 - accuracy: 0.0989 - val_loss: 2.4782 - val_accuracy: 0.1046\n",
      "Epoch 3/170\n",
      "5/5 [==============================] - 4s 840ms/step - loss: 2.4766 - accuracy: 0.1123 - val_loss: 2.4656 - val_accuracy: 0.1232\n",
      "Epoch 4/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 2.4590 - accuracy: 0.1351 - val_loss: 2.4361 - val_accuracy: 0.1541\n",
      "Epoch 5/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 2.4146 - accuracy: 0.1706 - val_loss: 2.3649 - val_accuracy: 0.1838\n",
      "Epoch 6/170\n",
      "5/5 [==============================] - 3s 707ms/step - loss: 2.3275 - accuracy: 0.2135 - val_loss: 2.1984 - val_accuracy: 0.2706\n",
      "Epoch 7/170\n",
      "5/5 [==============================] - 4s 737ms/step - loss: 2.1607 - accuracy: 0.3056 - val_loss: 2.1164 - val_accuracy: 0.3153\n",
      "Epoch 8/170\n",
      "5/5 [==============================] - 3s 744ms/step - loss: 2.0168 - accuracy: 0.3444 - val_loss: 2.0584 - val_accuracy: 0.3190\n",
      "Epoch 9/170\n",
      "5/5 [==============================] - 3s 728ms/step - loss: 2.0095 - accuracy: 0.3469 - val_loss: 1.9874 - val_accuracy: 0.3211\n",
      "Epoch 10/170\n",
      "5/5 [==============================] - 3s 720ms/step - loss: 1.8883 - accuracy: 0.3534 - val_loss: 1.9649 - val_accuracy: 0.3196\n",
      "Epoch 11/170\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 1.8572 - accuracy: 0.3514 - val_loss: 1.9479 - val_accuracy: 0.3238\n",
      "Epoch 12/170\n",
      "5/5 [==============================] - 3s 705ms/step - loss: 1.8160 - accuracy: 0.3594 - val_loss: 1.9273 - val_accuracy: 0.3354\n",
      "Epoch 13/170\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 1.8605 - accuracy: 0.3632 - val_loss: 1.9334 - val_accuracy: 0.3395\n",
      "Epoch 14/170\n",
      "5/5 [==============================] - 3s 754ms/step - loss: 1.7904 - accuracy: 0.3729 - val_loss: 1.8820 - val_accuracy: 0.3439\n",
      "Epoch 15/170\n",
      "5/5 [==============================] - 4s 731ms/step - loss: 1.7766 - accuracy: 0.3703 - val_loss: 1.8560 - val_accuracy: 0.3509\n",
      "Epoch 16/170\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 1.7271 - accuracy: 0.3840 - val_loss: 1.8284 - val_accuracy: 0.3719\n",
      "Epoch 17/170\n",
      "5/5 [==============================] - 4s 743ms/step - loss: 1.6820 - accuracy: 0.4040 - val_loss: 1.7967 - val_accuracy: 0.3932\n",
      "Epoch 18/170\n",
      "5/5 [==============================] - 3s 723ms/step - loss: 1.6193 - accuracy: 0.4303 - val_loss: 1.7440 - val_accuracy: 0.4265\n",
      "Epoch 19/170\n",
      "5/5 [==============================] - 4s 772ms/step - loss: 1.5397 - accuracy: 0.4752 - val_loss: 1.6769 - val_accuracy: 0.4582\n",
      "Epoch 20/170\n",
      "5/5 [==============================] - 4s 767ms/step - loss: 1.4939 - accuracy: 0.5109 - val_loss: 1.6494 - val_accuracy: 0.4591\n",
      "Epoch 21/170\n",
      "5/5 [==============================] - 4s 750ms/step - loss: 1.5180 - accuracy: 0.5145 - val_loss: 1.5762 - val_accuracy: 0.5100\n",
      "Epoch 22/170\n",
      "5/5 [==============================] - 4s 741ms/step - loss: 1.3169 - accuracy: 0.6115 - val_loss: 1.4603 - val_accuracy: 0.5519\n",
      "Epoch 23/170\n",
      "5/5 [==============================] - 4s 733ms/step - loss: 1.2119 - accuracy: 0.6474 - val_loss: 1.4319 - val_accuracy: 0.5608\n",
      "Epoch 24/170\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 1.2175 - accuracy: 0.6402 - val_loss: 1.3840 - val_accuracy: 0.5738\n",
      "Epoch 25/170\n",
      "5/5 [==============================] - 4s 762ms/step - loss: 1.1967 - accuracy: 0.6503 - val_loss: 1.3191 - val_accuracy: 0.5681\n",
      "Epoch 26/170\n",
      "5/5 [==============================] - 4s 767ms/step - loss: 1.0980 - accuracy: 0.6717 - val_loss: 1.3622 - val_accuracy: 0.5575\n",
      "Epoch 27/170\n",
      "5/5 [==============================] - 4s 743ms/step - loss: 1.1629 - accuracy: 0.6487 - val_loss: 1.2628 - val_accuracy: 0.5856\n",
      "Epoch 28/170\n",
      "5/5 [==============================] - 4s 734ms/step - loss: 1.1409 - accuracy: 0.6534 - val_loss: 1.2153 - val_accuracy: 0.5832\n",
      "Epoch 29/170\n",
      "5/5 [==============================] - 4s 731ms/step - loss: 1.0611 - accuracy: 0.6690 - val_loss: 1.2076 - val_accuracy: 0.5847\n",
      "Epoch 30/170\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 1.0299 - accuracy: 0.6742 - val_loss: 1.1719 - val_accuracy: 0.5915\n",
      "Epoch 31/170\n",
      "5/5 [==============================] - 4s 766ms/step - loss: 1.0879 - accuracy: 0.6639 - val_loss: 1.2064 - val_accuracy: 0.5782\n",
      "Epoch 32/170\n",
      "5/5 [==============================] - 3s 757ms/step - loss: 1.0083 - accuracy: 0.6810 - val_loss: 1.1784 - val_accuracy: 0.5823\n",
      "Epoch 33/170\n",
      "5/5 [==============================] - 4s 727ms/step - loss: 0.9850 - accuracy: 0.6831 - val_loss: 1.1736 - val_accuracy: 0.5845\n",
      "Epoch 34/170\n",
      "5/5 [==============================] - 4s 741ms/step - loss: 0.9970 - accuracy: 0.6810 - val_loss: 1.1210 - val_accuracy: 0.5988\n",
      "Epoch 35/170\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 1.0427 - accuracy: 0.6699 - val_loss: 1.2845 - val_accuracy: 0.5600\n",
      "Epoch 36/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 1.0055 - accuracy: 0.6782 - val_loss: 1.1453 - val_accuracy: 0.5902\n",
      "Epoch 37/170\n",
      "5/5 [==============================] - 4s 755ms/step - loss: 0.9580 - accuracy: 0.6884 - val_loss: 1.1286 - val_accuracy: 0.5942\n",
      "Epoch 38/170\n",
      "5/5 [==============================] - 3s 759ms/step - loss: 0.9678 - accuracy: 0.6848 - val_loss: 1.1384 - val_accuracy: 0.5925\n",
      "Epoch 39/170\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 0.9650 - accuracy: 0.6795 - val_loss: 1.1115 - val_accuracy: 0.5977\n",
      "Epoch 40/170\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.9546 - accuracy: 0.6861 - val_loss: 1.2063 - val_accuracy: 0.5732\n",
      "Epoch 41/170\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.9754 - accuracy: 0.6809 - val_loss: 1.0851 - val_accuracy: 0.6018\n",
      "Epoch 42/170\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.9422 - accuracy: 0.6900 - val_loss: 1.0984 - val_accuracy: 0.5966\n",
      "Epoch 43/170\n",
      "5/5 [==============================] - 4s 741ms/step - loss: 0.9191 - accuracy: 0.6958 - val_loss: 1.0787 - val_accuracy: 0.6013\n",
      "Epoch 44/170\n",
      "5/5 [==============================] - 3s 758ms/step - loss: 0.9423 - accuracy: 0.6832 - val_loss: 1.1134 - val_accuracy: 0.5937\n",
      "Epoch 45/170\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.9360 - accuracy: 0.6906 - val_loss: 1.0629 - val_accuracy: 0.6048\n",
      "Epoch 46/170\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.9090 - accuracy: 0.6927 - val_loss: 1.0874 - val_accuracy: 0.5983\n",
      "Epoch 47/170\n",
      "5/5 [==============================] - 3s 712ms/step - loss: 0.8986 - accuracy: 0.6985 - val_loss: 1.0496 - val_accuracy: 0.6058\n",
      "Epoch 48/170\n",
      "5/5 [==============================] - 3s 705ms/step - loss: 0.9125 - accuracy: 0.6940 - val_loss: 1.0851 - val_accuracy: 0.5976\n",
      "Epoch 49/170\n",
      "5/5 [==============================] - 4s 765ms/step - loss: 0.9062 - accuracy: 0.6955 - val_loss: 1.0448 - val_accuracy: 0.6055\n",
      "Epoch 50/170\n",
      "5/5 [==============================] - 4s 903ms/step - loss: 0.8944 - accuracy: 0.7016 - val_loss: 1.1004 - val_accuracy: 0.5964\n",
      "Epoch 51/170\n",
      "5/5 [==============================] - 4s 783ms/step - loss: 0.9308 - accuracy: 0.6918 - val_loss: 1.0397 - val_accuracy: 0.6078\n",
      "Epoch 52/170\n",
      "5/5 [==============================] - 4s 721ms/step - loss: 0.8803 - accuracy: 0.7059 - val_loss: 1.0228 - val_accuracy: 0.6126\n",
      "Epoch 53/170\n",
      "5/5 [==============================] - 3s 708ms/step - loss: 0.8642 - accuracy: 0.7088 - val_loss: 1.0173 - val_accuracy: 0.6161\n",
      "Epoch 54/170\n",
      "5/5 [==============================] - 3s 707ms/step - loss: 0.8632 - accuracy: 0.7113 - val_loss: 1.0181 - val_accuracy: 0.6189\n",
      "Epoch 55/170\n",
      "5/5 [==============================] - 4s 738ms/step - loss: 0.8736 - accuracy: 0.7102 - val_loss: 0.9947 - val_accuracy: 0.6288\n",
      "Epoch 56/170\n",
      "5/5 [==============================] - 3s 751ms/step - loss: 1.0311 - accuracy: 0.6781 - val_loss: 1.0686 - val_accuracy: 0.6226\n",
      "Epoch 57/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.9107 - accuracy: 0.7051 - val_loss: 0.9937 - val_accuracy: 0.6418\n",
      "Epoch 58/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.8559 - accuracy: 0.7249 - val_loss: 0.9794 - val_accuracy: 0.6580\n",
      "Epoch 59/170\n",
      "5/5 [==============================] - 4s 730ms/step - loss: 0.8379 - accuracy: 0.7337 - val_loss: 0.9627 - val_accuracy: 0.6704\n",
      "Epoch 60/170\n",
      "5/5 [==============================] - 3s 707ms/step - loss: 0.8340 - accuracy: 0.7330 - val_loss: 0.9397 - val_accuracy: 0.6749\n",
      "Epoch 61/170\n",
      "5/5 [==============================] - 4s 740ms/step - loss: 0.8904 - accuracy: 0.7197 - val_loss: 0.9312 - val_accuracy: 0.6841\n",
      "Epoch 62/170\n",
      "5/5 [==============================] - 3s 743ms/step - loss: 0.8308 - accuracy: 0.7398 - val_loss: 0.8974 - val_accuracy: 0.7125\n",
      "Epoch 63/170\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.8152 - accuracy: 0.7497 - val_loss: 0.8899 - val_accuracy: 0.7146\n",
      "Epoch 64/170\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.8051 - accuracy: 0.7509 - val_loss: 0.8733 - val_accuracy: 0.7171\n",
      "Epoch 65/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.7839 - accuracy: 0.7564 - val_loss: 0.8542 - val_accuracy: 0.7201\n",
      "Epoch 66/170\n",
      "5/5 [==============================] - 3s 701ms/step - loss: 0.8258 - accuracy: 0.7435 - val_loss: 0.8840 - val_accuracy: 0.7192\n",
      "Epoch 67/170\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 0.7828 - accuracy: 0.7571 - val_loss: 0.8406 - val_accuracy: 0.7245\n",
      "Epoch 68/170\n",
      "5/5 [==============================] - 3s 750ms/step - loss: 0.7891 - accuracy: 0.7550 - val_loss: 0.8535 - val_accuracy: 0.7232\n",
      "Epoch 69/170\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.7798 - accuracy: 0.7567 - val_loss: 0.8676 - val_accuracy: 0.7177\n",
      "Epoch 70/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.7868 - accuracy: 0.7566 - val_loss: 0.8299 - val_accuracy: 0.7208\n",
      "Epoch 71/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.7580 - accuracy: 0.7643 - val_loss: 0.8048 - val_accuracy: 0.7363\n",
      "Epoch 72/170\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.7415 - accuracy: 0.7727 - val_loss: 0.8252 - val_accuracy: 0.7334\n",
      "Epoch 73/170\n",
      "5/5 [==============================] - 4s 739ms/step - loss: 0.7263 - accuracy: 0.7785 - val_loss: 0.8374 - val_accuracy: 0.7322\n",
      "Epoch 74/170\n",
      "5/5 [==============================] - 4s 860ms/step - loss: 0.7895 - accuracy: 0.7580 - val_loss: 0.9306 - val_accuracy: 0.7120\n",
      "Epoch 75/170\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.7731 - accuracy: 0.7662 - val_loss: 0.8121 - val_accuracy: 0.7430\n",
      "Epoch 76/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.7170 - accuracy: 0.7829 - val_loss: 0.7883 - val_accuracy: 0.7496\n",
      "Epoch 77/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.7014 - accuracy: 0.7916 - val_loss: 0.7645 - val_accuracy: 0.7532\n",
      "Epoch 78/170\n",
      "5/5 [==============================] - 3s 705ms/step - loss: 0.7003 - accuracy: 0.7900 - val_loss: 0.8482 - val_accuracy: 0.7281\n",
      "Epoch 79/170\n",
      "5/5 [==============================] - 4s 737ms/step - loss: 0.7254 - accuracy: 0.7839 - val_loss: 0.7613 - val_accuracy: 0.7539\n",
      "Epoch 80/170\n",
      "5/5 [==============================] - 3s 750ms/step - loss: 0.7278 - accuracy: 0.7861 - val_loss: 0.8146 - val_accuracy: 0.7425\n",
      "Epoch 81/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.7178 - accuracy: 0.7851 - val_loss: 0.7403 - val_accuracy: 0.7609\n",
      "Epoch 82/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.6774 - accuracy: 0.7997 - val_loss: 0.7355 - val_accuracy: 0.7668\n",
      "Epoch 83/170\n",
      "5/5 [==============================] - 3s 708ms/step - loss: 0.6786 - accuracy: 0.7987 - val_loss: 0.8072 - val_accuracy: 0.7483\n",
      "Epoch 84/170\n",
      "5/5 [==============================] - 3s 702ms/step - loss: 0.6982 - accuracy: 0.7928 - val_loss: 0.7193 - val_accuracy: 0.7677\n",
      "Epoch 85/170\n",
      "5/5 [==============================] - 4s 753ms/step - loss: 0.6750 - accuracy: 0.7999 - val_loss: 0.7528 - val_accuracy: 0.7611\n",
      "Epoch 86/170\n",
      "5/5 [==============================] - 3s 743ms/step - loss: 0.6799 - accuracy: 0.7990 - val_loss: 0.7441 - val_accuracy: 0.7633\n",
      "Epoch 87/170\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.6509 - accuracy: 0.8076 - val_loss: 0.7189 - val_accuracy: 0.7747\n",
      "Epoch 88/170\n",
      "5/5 [==============================] - 4s 823ms/step - loss: 0.6525 - accuracy: 0.8071 - val_loss: 0.7326 - val_accuracy: 0.7557\n",
      "Epoch 89/170\n",
      "5/5 [==============================] - 4s 821ms/step - loss: 0.6685 - accuracy: 0.8005 - val_loss: 0.8139 - val_accuracy: 0.7415\n",
      "Epoch 90/170\n",
      "5/5 [==============================] - 4s 732ms/step - loss: 0.6746 - accuracy: 0.8006 - val_loss: 0.7114 - val_accuracy: 0.7812\n",
      "Epoch 91/170\n",
      "5/5 [==============================] - 4s 783ms/step - loss: 0.6402 - accuracy: 0.8118 - val_loss: 0.6959 - val_accuracy: 0.7838\n",
      "Epoch 92/170\n",
      "5/5 [==============================] - 3s 746ms/step - loss: 0.6297 - accuracy: 0.8141 - val_loss: 0.6930 - val_accuracy: 0.7841\n",
      "Epoch 93/170\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.6472 - accuracy: 0.8088 - val_loss: 0.7452 - val_accuracy: 0.7662\n",
      "Epoch 94/170\n",
      "5/5 [==============================] - 3s 712ms/step - loss: 0.6375 - accuracy: 0.8124 - val_loss: 0.6628 - val_accuracy: 0.7963\n",
      "Epoch 95/170\n",
      "5/5 [==============================] - 3s 713ms/step - loss: 0.6846 - accuracy: 0.7973 - val_loss: 0.6867 - val_accuracy: 0.7982\n",
      "Epoch 96/170\n",
      "5/5 [==============================] - 3s 702ms/step - loss: 0.6309 - accuracy: 0.8149 - val_loss: 0.6759 - val_accuracy: 0.7936\n",
      "Epoch 97/170\n",
      "5/5 [==============================] - 4s 737ms/step - loss: 0.6204 - accuracy: 0.8191 - val_loss: 0.6700 - val_accuracy: 0.8076\n",
      "Epoch 98/170\n",
      "5/5 [==============================] - 3s 745ms/step - loss: 0.6262 - accuracy: 0.8172 - val_loss: 0.6838 - val_accuracy: 0.7859\n",
      "Epoch 99/170\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.6208 - accuracy: 0.8180 - val_loss: 0.6564 - val_accuracy: 0.8072\n",
      "Epoch 100/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.6152 - accuracy: 0.8192 - val_loss: 0.6435 - val_accuracy: 0.8059\n",
      "Epoch 101/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.6095 - accuracy: 0.8207 - val_loss: 0.7061 - val_accuracy: 0.7970\n",
      "Epoch 102/170\n",
      "5/5 [==============================] - 3s 711ms/step - loss: 0.6210 - accuracy: 0.8192 - val_loss: 0.6448 - val_accuracy: 0.8182\n",
      "Epoch 103/170\n",
      "5/5 [==============================] - 4s 740ms/step - loss: 0.6099 - accuracy: 0.8231 - val_loss: 0.7029 - val_accuracy: 0.7874\n",
      "Epoch 104/170\n",
      "5/5 [==============================] - 3s 748ms/step - loss: 0.5973 - accuracy: 0.8267 - val_loss: 0.6258 - val_accuracy: 0.8247\n",
      "Epoch 105/170\n",
      "5/5 [==============================] - 3s 724ms/step - loss: 0.6046 - accuracy: 0.8259 - val_loss: 0.6377 - val_accuracy: 0.8187\n",
      "Epoch 106/170\n",
      "5/5 [==============================] - 3s 711ms/step - loss: 0.5846 - accuracy: 0.8315 - val_loss: 0.6437 - val_accuracy: 0.8213\n",
      "Epoch 107/170\n",
      "5/5 [==============================] - 3s 709ms/step - loss: 0.5956 - accuracy: 0.8269 - val_loss: 0.6149 - val_accuracy: 0.8250\n",
      "Epoch 108/170\n",
      "5/5 [==============================] - 3s 706ms/step - loss: 0.5946 - accuracy: 0.8270 - val_loss: 0.6444 - val_accuracy: 0.8118\n",
      "Epoch 109/170\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 0.6021 - accuracy: 0.8261 - val_loss: 0.6351 - val_accuracy: 0.8232\n",
      "Epoch 110/170\n",
      "5/5 [==============================] - 3s 741ms/step - loss: 0.6082 - accuracy: 0.8228 - val_loss: 0.6140 - val_accuracy: 0.8257\n",
      "Epoch 111/170\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.5840 - accuracy: 0.8299 - val_loss: 0.5912 - val_accuracy: 0.8363\n",
      "Epoch 112/170\n",
      "5/5 [==============================] - 3s 712ms/step - loss: 0.5921 - accuracy: 0.8276 - val_loss: 0.6352 - val_accuracy: 0.8238\n",
      "Epoch 113/170\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 0.5717 - accuracy: 0.8362 - val_loss: 0.5930 - val_accuracy: 0.8362\n",
      "Epoch 114/170\n",
      "5/5 [==============================] - 3s 704ms/step - loss: 0.5683 - accuracy: 0.8371 - val_loss: 0.6636 - val_accuracy: 0.8099\n",
      "Epoch 115/170\n",
      "5/5 [==============================] - 4s 748ms/step - loss: 0.5925 - accuracy: 0.8285 - val_loss: 0.6012 - val_accuracy: 0.8300\n",
      "Epoch 116/170\n",
      "5/5 [==============================] - 3s 756ms/step - loss: 0.5675 - accuracy: 0.8356 - val_loss: 0.6134 - val_accuracy: 0.8314\n",
      "Epoch 117/170\n",
      "5/5 [==============================] - 3s 721ms/step - loss: 0.5820 - accuracy: 0.8319 - val_loss: 0.5981 - val_accuracy: 0.8259\n",
      "Epoch 118/170\n",
      "5/5 [==============================] - 3s 722ms/step - loss: 0.5891 - accuracy: 0.8270 - val_loss: 0.5894 - val_accuracy: 0.8357\n",
      "Epoch 119/170\n",
      "5/5 [==============================] - 3s 713ms/step - loss: 0.5568 - accuracy: 0.8391 - val_loss: 0.5954 - val_accuracy: 0.8336\n",
      "Epoch 120/170\n",
      "5/5 [==============================] - 3s 699ms/step - loss: 0.5603 - accuracy: 0.8379 - val_loss: 0.5934 - val_accuracy: 0.8358\n",
      "Epoch 121/170\n",
      "5/5 [==============================] - 4s 730ms/step - loss: 0.5646 - accuracy: 0.8371 - val_loss: 0.7452 - val_accuracy: 0.7841\n",
      "Epoch 122/170\n",
      "5/5 [==============================] - 3s 748ms/step - loss: 0.6006 - accuracy: 0.8278 - val_loss: 0.5789 - val_accuracy: 0.8398\n",
      "Epoch 123/170\n",
      "5/5 [==============================] - 3s 725ms/step - loss: 0.5715 - accuracy: 0.8340 - val_loss: 0.5848 - val_accuracy: 0.8368\n",
      "Epoch 124/170\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.5586 - accuracy: 0.8372 - val_loss: 0.5731 - val_accuracy: 0.8416\n",
      "Epoch 125/170\n",
      "5/5 [==============================] - 3s 728ms/step - loss: 0.5585 - accuracy: 0.8377 - val_loss: 0.6006 - val_accuracy: 0.8325\n",
      "Epoch 126/170\n",
      "5/5 [==============================] - 3s 701ms/step - loss: 0.5378 - accuracy: 0.8448 - val_loss: 0.5739 - val_accuracy: 0.8389\n",
      "Epoch 127/170\n",
      "5/5 [==============================] - 4s 733ms/step - loss: 0.5427 - accuracy: 0.8427 - val_loss: 0.5740 - val_accuracy: 0.8398\n",
      "Epoch 128/170\n",
      "5/5 [==============================] - 3s 752ms/step - loss: 0.5499 - accuracy: 0.8399 - val_loss: 0.6097 - val_accuracy: 0.8247\n",
      "Epoch 129/170\n",
      "5/5 [==============================] - 3s 717ms/step - loss: 0.5881 - accuracy: 0.8275 - val_loss: 0.5903 - val_accuracy: 0.8308\n",
      "Epoch 130/170\n",
      "5/5 [==============================] - 3s 707ms/step - loss: 0.5502 - accuracy: 0.8411 - val_loss: 0.5757 - val_accuracy: 0.8390\n",
      "Epoch 131/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.5372 - accuracy: 0.8443 - val_loss: 0.5610 - val_accuracy: 0.8423\n",
      "Epoch 132/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.5394 - accuracy: 0.8428 - val_loss: 0.6241 - val_accuracy: 0.8248\n",
      "Epoch 133/170\n",
      "5/5 [==============================] - 4s 749ms/step - loss: 0.5520 - accuracy: 0.8406 - val_loss: 0.5692 - val_accuracy: 0.8394\n",
      "Epoch 134/170\n",
      "5/5 [==============================] - 3s 745ms/step - loss: 0.5423 - accuracy: 0.8427 - val_loss: 0.5741 - val_accuracy: 0.8378\n",
      "Epoch 135/170\n",
      "5/5 [==============================] - 3s 727ms/step - loss: 0.5321 - accuracy: 0.8450 - val_loss: 0.5535 - val_accuracy: 0.8445\n",
      "Epoch 136/170\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.5330 - accuracy: 0.8447 - val_loss: 0.5813 - val_accuracy: 0.8368\n",
      "Epoch 137/170\n",
      "5/5 [==============================] - 4s 731ms/step - loss: 0.5306 - accuracy: 0.8449 - val_loss: 0.5518 - val_accuracy: 0.8429\n",
      "Epoch 138/170\n",
      "5/5 [==============================] - 3s 707ms/step - loss: 0.5284 - accuracy: 0.8454 - val_loss: 0.5763 - val_accuracy: 0.8348\n",
      "Epoch 139/170\n",
      "5/5 [==============================] - 4s 733ms/step - loss: 0.5261 - accuracy: 0.8462 - val_loss: 0.5470 - val_accuracy: 0.8456\n",
      "Epoch 140/170\n",
      "5/5 [==============================] - 3s 739ms/step - loss: 0.5281 - accuracy: 0.8451 - val_loss: 0.6440 - val_accuracy: 0.8171\n",
      "Epoch 141/170\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.5909 - accuracy: 0.8271 - val_loss: 0.5670 - val_accuracy: 0.8386\n",
      "Epoch 142/170\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.5302 - accuracy: 0.8458 - val_loss: 0.5502 - val_accuracy: 0.8447\n",
      "Epoch 143/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.5281 - accuracy: 0.8455 - val_loss: 0.5459 - val_accuracy: 0.8464\n",
      "Epoch 144/170\n",
      "5/5 [==============================] - 3s 708ms/step - loss: 0.5310 - accuracy: 0.8448 - val_loss: 0.5698 - val_accuracy: 0.8379\n",
      "Epoch 145/170\n",
      "5/5 [==============================] - 4s 737ms/step - loss: 0.5147 - accuracy: 0.8506 - val_loss: 0.5517 - val_accuracy: 0.8436\n",
      "Epoch 146/170\n",
      "5/5 [==============================] - 3s 749ms/step - loss: 0.5187 - accuracy: 0.8488 - val_loss: 0.5707 - val_accuracy: 0.8338\n",
      "Epoch 147/170\n",
      "5/5 [==============================] - 3s 718ms/step - loss: 0.5276 - accuracy: 0.8436 - val_loss: 0.5540 - val_accuracy: 0.8417\n",
      "Epoch 148/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.5281 - accuracy: 0.8456 - val_loss: 0.5669 - val_accuracy: 0.8349\n",
      "Epoch 149/170\n",
      "5/5 [==============================] - 3s 713ms/step - loss: 0.5172 - accuracy: 0.8489 - val_loss: 0.5544 - val_accuracy: 0.8415\n",
      "Epoch 150/170\n",
      "5/5 [==============================] - 3s 712ms/step - loss: 0.5062 - accuracy: 0.8524 - val_loss: 0.5938 - val_accuracy: 0.8279\n",
      "Epoch 151/170\n",
      "5/5 [==============================] - 4s 736ms/step - loss: 0.5232 - accuracy: 0.8465 - val_loss: 0.5271 - val_accuracy: 0.8487\n",
      "Epoch 152/170\n",
      "5/5 [==============================] - 3s 750ms/step - loss: 0.5247 - accuracy: 0.8456 - val_loss: 0.5462 - val_accuracy: 0.8440\n",
      "Epoch 153/170\n",
      "5/5 [==============================] - 3s 715ms/step - loss: 0.5148 - accuracy: 0.8498 - val_loss: 0.5260 - val_accuracy: 0.8494\n",
      "Epoch 154/170\n",
      "5/5 [==============================] - 3s 726ms/step - loss: 0.5156 - accuracy: 0.8491 - val_loss: 0.5718 - val_accuracy: 0.8365\n",
      "Epoch 155/170\n",
      "5/5 [==============================] - 3s 712ms/step - loss: 0.5156 - accuracy: 0.8498 - val_loss: 0.5465 - val_accuracy: 0.8442\n",
      "Epoch 156/170\n",
      "5/5 [==============================] - 3s 702ms/step - loss: 0.5038 - accuracy: 0.8533 - val_loss: 0.5550 - val_accuracy: 0.8409\n",
      "Epoch 157/170\n",
      "5/5 [==============================] - 4s 746ms/step - loss: 0.5122 - accuracy: 0.8495 - val_loss: 0.5490 - val_accuracy: 0.8432\n",
      "Epoch 158/170\n",
      "5/5 [==============================] - 3s 745ms/step - loss: 0.5153 - accuracy: 0.8486 - val_loss: 0.5568 - val_accuracy: 0.8381\n",
      "Epoch 159/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.5446 - accuracy: 0.8374 - val_loss: 0.5409 - val_accuracy: 0.8442\n",
      "Epoch 160/170\n",
      "5/5 [==============================] - 3s 716ms/step - loss: 0.5171 - accuracy: 0.8474 - val_loss: 0.6038 - val_accuracy: 0.8260\n",
      "Epoch 161/170\n",
      "5/5 [==============================] - 3s 710ms/step - loss: 0.5299 - accuracy: 0.8448 - val_loss: 0.5226 - val_accuracy: 0.8491\n",
      "Epoch 162/170\n",
      "5/5 [==============================] - 3s 704ms/step - loss: 0.5165 - accuracy: 0.8490 - val_loss: 0.5489 - val_accuracy: 0.8428\n",
      "Epoch 163/170\n",
      "5/5 [==============================] - 4s 743ms/step - loss: 0.5050 - accuracy: 0.8523 - val_loss: 0.5257 - val_accuracy: 0.8494\n",
      "Epoch 164/170\n",
      "5/5 [==============================] - 3s 746ms/step - loss: 0.4873 - accuracy: 0.8558 - val_loss: 0.5356 - val_accuracy: 0.8452\n",
      "Epoch 165/170\n",
      "5/5 [==============================] - 3s 719ms/step - loss: 0.5101 - accuracy: 0.8492 - val_loss: 0.5318 - val_accuracy: 0.8462\n",
      "Epoch 166/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.5024 - accuracy: 0.8519 - val_loss: 0.5585 - val_accuracy: 0.8381\n",
      "Epoch 167/170\n",
      "5/5 [==============================] - 3s 714ms/step - loss: 0.5053 - accuracy: 0.8527 - val_loss: 0.5187 - val_accuracy: 0.8494\n",
      "Epoch 168/170\n",
      "5/5 [==============================] - 3s 702ms/step - loss: 0.4892 - accuracy: 0.8567 - val_loss: 0.5555 - val_accuracy: 0.8400\n",
      "Epoch 169/170\n",
      "5/5 [==============================] - 4s 742ms/step - loss: 0.5040 - accuracy: 0.8529 - val_loss: 0.5137 - val_accuracy: 0.8508\n",
      "Epoch 170/170\n",
      "5/5 [==============================] - 3s 749ms/step - loss: 0.4956 - accuracy: 0.8534 - val_loss: 0.5430 - val_accuracy: 0.8421\n"
     ]
    }
   ],
   "source": [
    "# number of training images\n",
    "train_count = 367\n",
    "\n",
    "# number of validation images\n",
    "validation_count = 101\n",
    "\n",
    "EPOCHS = 170\n",
    "\n",
    "steps_per_epoch = train_count//BATCH_SIZE\n",
    "validation_steps = validation_count//BATCH_SIZE\n",
    "\n",
    "history = model.fit(training_dataset,\n",
    "                    steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zENjQuK0luH5"
   },
   "outputs": [],
   "source": [
    "def get_images_and_segments_test_arrays():\n",
    "  '''\n",
    "  Gets a subsample of the val set as your test set\n",
    "\n",
    "  Returns:\n",
    "    Test set containing ground truth images and label maps\n",
    "  '''\n",
    "  y_true_segments = []\n",
    "  y_true_images = []\n",
    "  test_count = 64\n",
    "\n",
    "  ds = validation_dataset.unbatch()\n",
    "  ds = ds.batch(101)\n",
    "\n",
    "  for image, annotation in ds.take(1):\n",
    "    y_true_images = image\n",
    "    y_true_segments = annotation\n",
    "\n",
    "\n",
    "  y_true_segments = y_true_segments[:test_count, : ,: , :]\n",
    "  y_true_segments = np.argmax(y_true_segments, axis=3)  \n",
    "\n",
    "  return y_true_images, y_true_segments\n",
    "\n",
    "# load the ground truth images and segmentation masks\n",
    "y_true_images, y_true_segments = get_images_and_segments_test_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "1CpEeUiN7ey9"
   },
   "outputs": [],
   "source": [
    "# get the model prediction\n",
    "results = model.predict(validation_dataset, steps=validation_steps)\n",
    "\n",
    "# for each pixel, get the slice number which has the highest probability\n",
    "results = np.argmax(results, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EobztGe_66sA"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "  '''\n",
    "  Computes IOU and Dice Score.\n",
    "\n",
    "  Args:\n",
    "    y_true (tensor) - ground truth label map\n",
    "    y_pred (tensor) - predicted label map\n",
    "  '''\n",
    "  \n",
    "  class_wise_iou = []\n",
    "  class_wise_dice_score = []\n",
    "\n",
    "  smoothening_factor = 0.00001\n",
    "\n",
    "  for i in range(12):\n",
    "    intersection = np.sum((y_pred == i) * (y_true == i))\n",
    "    y_true_area = np.sum((y_true == i))\n",
    "    y_pred_area = np.sum((y_pred == i))\n",
    "    combined_area = y_true_area + y_pred_area\n",
    "    \n",
    "    iou = (intersection + smoothening_factor) / (combined_area - intersection + smoothening_factor)\n",
    "    class_wise_iou.append(iou)\n",
    "    \n",
    "    dice_score =  2 * ((intersection + smoothening_factor) / (combined_area + smoothening_factor))\n",
    "    class_wise_dice_score.append(dice_score)\n",
    "\n",
    "  return class_wise_iou, class_wise_dice_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XqzDRh0e6_8G"
   },
   "outputs": [],
   "source": [
    "# compute class-wise metrics\n",
    "cls_wise_iou, cls_wise_dice_score = compute_metrics(y_true_segments, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7mnS0UPtsMeB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky            0.8854171947113912 \n",
      "building       0.7625224531579848 \n",
      "column/pole    4.431293479668856e-05 \n",
      "road           0.8898122214816246 \n",
      "side walk      0.6841891676541243 \n",
      "vegetation     0.820562710151038 \n",
      "traffic light  0.0001536667895526256 \n",
      "fence          0.002202529924331519 \n",
      "vehicle        0.31466674960739455 \n",
      "pedestrian     0.09353176204033675 \n",
      "byciclist      0.03745124223600098 \n",
      "void           0.06752986755488875 \n"
     ]
    }
   ],
   "source": [
    "# print IOU for each class\n",
    "for idx, iou in enumerate(cls_wise_iou):\n",
    "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
    "  print(\"{}{}{} \".format(class_names[idx], spaces, iou)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "MVxYk02pJm8O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sky            0.9392268164413388 \n",
      "building       0.8652626828059327 \n",
      "column/pole    8.862194253428664e-05 \n",
      "road           0.9416937951524464 \n",
      "side walk      0.8124849402978815 \n",
      "vegetation     0.9014385558740531 \n",
      "traffic light  0.0003072863594913541 \n",
      "fence          0.004395378895569152 \n",
      "vehicle        0.4787019215573017 \n",
      "pedestrian     0.17106364042303987 \n",
      "byciclist      0.07219855876785308 \n",
      "void           0.12651611841070604 \n"
     ]
    }
   ],
   "source": [
    "# print the dice score for each class\n",
    "for idx, dice_score in enumerate(cls_wise_dice_score):\n",
    "  spaces = ' ' * (13-len(class_names[idx]) + 2)\n",
    "  print(\"{}{}{} \".format(class_names[idx], spaces, dice_score)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  3 16:25:04 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:18:00.0 Off |                    0 |\n",
      "| N/A   49C    P0    61W / 300W |  31022MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     44387      C   ...nda/envs/my_tf/bin/python    31017MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cg26-1.agave.rc.asu.edu\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W3_Lab_1_VGG16-FCN8-CamVid.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
